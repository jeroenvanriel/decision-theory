{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 925,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total capacity of the queue\n",
    "Qmax = 5\n",
    "\n",
    "# allowed actions an associated costs\n",
    "actions = [0,    1,    2,    3]\n",
    "C =       [0, -0.5, -2.5, -7.0]\n",
    "\n",
    "# holding costs\n",
    "Chold = [0, -1, -2, -5, -7, -10]\n",
    "\n",
    "# distribution of arrivals (make sure to specify for \n",
    "# all [0, ..., Qmax] to avoid problems with np.put())\n",
    "A = np.array([0.5, 0.0, 0.4, 0, 0, 0.1])\n",
    "\n",
    "# construct the transition matrix for action 0...\n",
    "p0 = np.zeros((Qmax + 1, Qmax + 1))\n",
    "for i in range(Qmax+1):\n",
    "    indices = range(i, Qmax)\n",
    "    values = A[0:Qmax - i]\n",
    "    np.put(p0[i], indices, values)\n",
    "\n",
    "    # last entry is probability of reaching full capacity\n",
    "    p0[i, Qmax] = sum(A[Qmax - i:Qmax+1])\n",
    "\n",
    "# ...from which transitions for other actions are simply derived\n",
    "def p(a, i, j):\n",
    "    return p0[max(i - a, 0), j]\n",
    "\n",
    "# rewards\n",
    "def r(a, i):\n",
    "    return Chold[i] + C[a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 926,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default policy (always take action 1)\n",
    "policy0 = np.ones((Qmax + 1), dtype=np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 963,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_policy(pol):\n",
    "    # compute rewards induced by policy\n",
    "    rf = np.zeros((Qmax + 1, 1))\n",
    "    for i in range(Qmax + 1):\n",
    "        rf[i, 0] = r(pol[i], i)\n",
    "    \n",
    "    # compute transitions induced by policy\n",
    "    pf = np.zeros((Qmax + 1, Qmax + 1))\n",
    "    for i in range(Qmax + 1):\n",
    "        for j in range(Qmax + 1):\n",
    "            pf[i, j] = p(pol[i], i, j)\n",
    "    \n",
    "    return rf, pf\n",
    "\n",
    "# evaluate policy using average reward criterium by solving (2.16)\n",
    "def evaluate_avg(pol):\n",
    "    rf, pf = apply_policy(pol)\n",
    "    \n",
    "    # solve system (2.16) using least squares\n",
    "    # (because the system is rank-deficient)\n",
    "    a = np.hstack([pf - np.eye(Qmax + 1), -np.ones((Qmax + 1, 1))])\n",
    "    b = - rf\n",
    "    x, _, _, _ = np.linalg.lstsq(a, b, rcond=None)\n",
    "    d = x[0:Qmax+1, 0]\n",
    "    g = x[-1, 0]\n",
    "    return d, g\n",
    "\n",
    "# evaluate policy using discounted total reward criterium\n",
    "# by solving (3.3)\n",
    "def evaluate_disc(pol, alpha=0.95):\n",
    "    rf, pf = apply_policy(pol)\n",
    "\n",
    "    a = alpha * pf - np.eye(Qmax + 1)\n",
    "    b = - rf\n",
    "    v = np.linalg.solve(a, b)\n",
    "    return v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 964,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-6.480113636363644\n"
     ]
    }
   ],
   "source": [
    "d0, g0 = evaluate_avg(policy0)\n",
    "print(g0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 965,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-111.34369133]\n",
      " [-112.34369133]\n",
      " [-117.08868158]\n",
      " [-125.69391909]\n",
      " [-134.5133063 ]\n",
      " [-141.70251522]]\n"
     ]
    }
   ],
   "source": [
    "v0 = evaluate_disc(policy0, 0.95)\n",
    "print(v0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 966,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-6.4799079 ]\n",
      " [-6.4799179 ]\n",
      " [-6.47997181]\n",
      " [-6.48006908]\n",
      " [-6.48016987]\n",
      " [-6.48025027]]\n"
     ]
    }
   ],
   "source": [
    "# verify avg and disc criteria by taking alpha close to 1\n",
    "alpha = 0.99999\n",
    "res = evaluate_disc(policy0, alpha) * (1 - alpha)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 967,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just using plain Python here, but we could have\n",
    "# used matrix multiplication for the second term\n",
    "def reward(v, i, a, alpha):\n",
    "    return r(a, i) + alpha * sum([\n",
    "        p(a, i, j) * v[j]\n",
    "        for j in range(Qmax + 1)\n",
    "    ])\n",
    "\n",
    "\n",
    "# apply one step of policy improvement\n",
    "# v is value or relative rewards\n",
    "# make sure to use alpha=1 in average reward case\n",
    "def policy_improv(policy, v, alpha=1):\n",
    "    new_policy = np.copy(policy)\n",
    "\n",
    "    # for all states, compute the optimal action\n",
    "    for i in range(Qmax + 1):\n",
    "        # current reward\n",
    "        best_reward = reward(v, i, policy[i], alpha)\n",
    "        for a in actions:\n",
    "            if a == policy[i]:\n",
    "                continue\n",
    "        \n",
    "            current = reward(v, i, a, alpha)\n",
    "            if current > best_reward:\n",
    "                best_reward = current\n",
    "                new_policy[i] = a\n",
    "\n",
    "    return new_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 961,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 2, 3, 3], dtype=int16)"
      ]
     },
     "execution_count": 961,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_improv(policy0, v0, alpha=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 962,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 2, 3, 3], dtype=int16)"
      ]
     },
     "execution_count": 962,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_improv(policy0, d0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 968,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1]\n",
      "-6.480113636363644\n",
      "[0 1 2 2 3 3]\n",
      "-4.2349999999999985\n",
      "[0 1 2 2 2 3]\n",
      "-4.208333333333335\n",
      "[0 1 2 2 2 3]\n"
     ]
    }
   ],
   "source": [
    "# average reward policy iteration\n",
    "policy = np.ones((Qmax + 1), dtype=np.int16)\n",
    "prev = None\n",
    "while not np.array_equal(policy, prev):\n",
    "    prev = np.copy(policy)\n",
    "    d, g = evaluate_avg(policy)\n",
    "    print(policy)\n",
    "    print(g)\n",
    "    policy = policy_improv(policy, d)\n",
    "\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 971,
   "metadata": {},
   "outputs": [],
   "source": [
    "# successive approximation\n",
    "def succ_approx(v0, epsilon=0.001):\n",
    "    v = np.copy(v0) + 2 * epsilon * np.ones_like(v0)\n",
    "    diff = 2 * epsilon\n",
    "    while diff > epsilon:\n",
    "        vprev = np.copy(v)\n",
    "        for i in range(Qmax + 1):\n",
    "            v[i] = max([reward(vprev, i, a, 1) for a in actions])\n",
    "            \n",
    "        M = np.max(np.abs(v - vprev))\n",
    "        m = np.min(np.abs(v - vprev))\n",
    "        diff = M - m\n",
    "    \n",
    "    # determine policy\n",
    "    policy = np.zeros_like(v0)\n",
    "    for i in range(Qmax + 1):\n",
    "        policy[i] = np.argmax([reward(v, i, a, 1) for a in actions])\n",
    "    \n",
    "    return v, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 973,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-42.74885045, -44.24885045, -47.24885045, -53.49864598,\n",
       "        -59.33174775, -66.83174775]),\n",
       " array([0., 1., 2., 2., 2., 3.]))"
      ]
     },
     "execution_count": 973,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "succ_approx(np.zeros((Qmax + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stoch-dec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
